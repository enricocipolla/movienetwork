{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = pd.DataFrame(columns = ['text_length_words', 'text_length_chars'])\n",
    "\n",
    "text_len['text_length_words'] = df['text'].apply(lambda x: len(x.split()))\n",
    "text_len['text_length_chars'] = df['text'].apply(len)\n",
    "text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('IMDB_movie_details.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([genre for genres in df.genre for genre in genres])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocessing(corpus): #revise this\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus = re.sub(r'[^\\w\\s]', '', corpus)\n",
    "    tokens = word_tokenize(corpus)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word.replace(',' , '') for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "df['processed'] = df.plot_summary.astype(str).apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "df['embeddings'] = model.encode([' '.join(sentence) for sentence in df['processed'].values])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "[G.add_node(index, title = row['title'], genre = row['genre'], plot_summary = row['plot_summary']) for index, row in df.iterrows()]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for i1, r1 in df[index+1:].iterrows():\n",
    "        cosim =  util.pytorch_cos_sim(row['embeddings'] , r1['embeddings'])\n",
    "        if cosim > 0.44:\n",
    "            G.add_edge(index, i1, weight = cosim[0][0])\n",
    "\n",
    "#remove nodes with no edges\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.remove_node(832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "degrees = [G.degree(n) * 100 for n in G.nodes()]  # Scale by 10 for better visualization\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw(G, node_size=degrees, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_d = nx.adjacency_matrix(G)\n",
    "print(A_d.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "A_d = nx.adjacency_matrix(G)\n",
    "sns.heatmap(A_d.todense(),\n",
    "            annot=True,\n",
    "            cmap = 'viridis',\n",
    "            xticklabels=G.nodes(),\n",
    "            yticklabels=G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betw_centr = dict(sorted(nx.betweenness_centrality(G).items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "print(betw_centr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_cent = dict(sorted(nx.closeness_centrality(G).items(), key=lambda x: x[1], reverse = True))\n",
    "close_cent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
